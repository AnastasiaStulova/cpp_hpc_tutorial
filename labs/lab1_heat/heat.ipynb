{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accelerating portable HPC Applications with Standard C++\n",
    "===\n",
    "\n",
    "# Lab 1: 2D Unsteady Heat Equation\n",
    "\n",
    "In this tutorial we will learn how to do multi-dimensional iteration in C++17 and C++23 and how to integrate parallel algorithms with pre-existing MPI applications, by accelerating a 2D heat equation solver (see slides).\n",
    "\n",
    "A working sequential implementation that does not use MPI is provided in [starting_point.cpp].\n",
    "Please take 5 minutes to skim through it.\n",
    "\n",
    "[starting_point.cpp]: ./starting_point.cpp\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Let's start by compiling and running the starting point:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -std=c++20 -Ofast -DNDEBUG -o heat starting_point.cpp\n",
    "\n",
    "# The binary takes the dimensions of the domain as two separate arguments: NX x NY and the number of iterations as the third parameter\n",
    "!./heat 1024 1024 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary writes a solution to an `output` file, that can be converted to a png file using the `vis` script or the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('dark_background') # Uncomment for dark background\n",
    "\n",
    "def visualize(name = 'output'):\n",
    "    f = open(name, 'rb')\n",
    "    grid = np.fromfile(f, dtype=np.uint64, count=2, offset=0)\n",
    "\n",
    "    nx = grid[0]\n",
    "    ny = grid[1]\n",
    "\n",
    "    times = np.fromfile(f, dtype=np.float64, count=1, offset=0)\n",
    "    time = times[0]\n",
    "\n",
    "    values = np.fromfile(f, dtype=np.float64, offset=0)\n",
    "    assert len(values) == nx * ny, f'{len(values)} != {nx * ny}'\n",
    "    values = values.reshape((nx, ny))\n",
    "\n",
    "    print(f'Plotting grid {nx}x{ny}, t = {time}')\n",
    "\n",
    "    plt.title(f'Temperature at t = {time:.3f} [s]')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.pcolormesh(values, cmap=plt.cm.jet, vmin=0, vmax=values.max())\n",
    "    plt.colorbar()\n",
    "    plt.savefig('output.png', transparent=True, bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 0: parallelize with C++ parallel algorithms and linear indexing\n",
    "\n",
    "The goal of this exercise is to parallelize the `stencil` and `initialize` implementations using the C++ parallel algorithms and the index `split`ing technique shown in the presentation.\n",
    "\n",
    "A template for the solution is provided in [exercise0.cpp].\n",
    "The only functions that needs to be modified to achieve this are the `stencil` and `initialize` functions.\n",
    "\n",
    "In the serial implementation, raw loops are used:\n",
    "\n",
    "```c++\n",
    "double stencil(double *u_new, double *u_old, grid g, parameters p) {\n",
    "  double energy = 0.;\n",
    "  for (long x = g.x_start; x < g.x_end; ++x) {\n",
    "    for (long y = g.y_start; y < g.y_end; ++y) {\n",
    "      energy += stencil(u_new, u_old, x, y, p);\n",
    "    }\n",
    "  }\n",
    "  return energy;\n",
    "}\n",
    "```\n",
    "\n",
    "Notice that there is already a function called `index` in the file, which maps 2D indices to 1D indices:\n",
    "\n",
    "```c++\n",
    "// Index into the memory using row-major order:\n",
    "long index(long x, long y, parameters p) {\n",
    "    assert(x >= 0 && x < p.nx);\n",
    "    assert(y >= 0 && y < p.ny);\n",
    "    return x * p.ny + y;\n",
    "};\n",
    "```\n",
    "\n",
    "When implementing the `stencil` API in the [exercise0.cpp] template, create a `split` function that is compatible with it:\n",
    "\n",
    "```c++\n",
    "double stencil(double *u_new, double *u_old, grid g, parameters p) {\n",
    "  double energy = 0.;\n",
    "  // TODO: implement using parallel algorithms\n",
    "  \n",
    "  auto split = [...](long idx) -> std::pair<long, long> {\n",
    "  \n",
    "  };\n",
    "  \n",
    "  // ...TODO...\n",
    "  \n",
    "  return energy;\n",
    "}\n",
    "```\n",
    "\n",
    "Recall that the goal for `split` and `index` is for the following invariant to hold:\n",
    "\n",
    "```c++\n",
    "auto [x1, y1] = split(index(x0, y0, p), p);\n",
    "assert(x0 == x1 && y0 == y1);\n",
    "```\n",
    "\n",
    "In the parallel implementation, you should use the C++ parallel algorithms discussed in the lecture:\n",
    "\n",
    "\n",
    "[exercise0.cpp]: ./exercise0.cpp\n",
    "\n",
    "While [exercise0.cpp] compiles and runs as provided, but it produces incorrect results due to the incomplete `stencil` and `initialize` implementations.\n",
    "Search for `TODO`s in the file and fix them until the following blocks compile and run correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -std=c++20 -Ofast -DNDEBUG -o heat exercise0.cpp -ltbb\n",
    "!./heat 1024 1024 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!clang++ -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat exercise0.cpp -ltbb\n",
    "!./heat 1024 1024 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -std=c++20 -stdpar=gpu -gpu=cc80 -fast -DNDEBUG -o heat exercise0.cpp\n",
    "!./heat 1024 1024 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions Exercise 0\n",
    "\n",
    "The solutions for each example are available in the [solutions/exercise0.cpp] file.\n",
    "\n",
    "[solutions/exercise0.cpp]: ./solutions/exercise0.cpp\n",
    "\n",
    "The following compiles and runs the solutions for Exercise 0 using different compilers and C++ standard versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!g++ -std=c++20 -Ofast -DNDEBUG -o heat solutions/exercise0.cpp -ltbb\n",
    "!./heat 1024 1024 10000\n",
    "!mv output output_gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!clang++ -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include -o heat solutions/exercise0.cpp -ltbb\n",
    "!./heat 1024 1024 10000\n",
    "!mv output output_clang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!nvc++ -std=c++20 -stdpar=gpu -gpu=cc80 -fast -DNDEBUG -o heat solutions/exercise0.cpp\n",
    "!./heat 1024 1024 10000\n",
    "!mv output output_nvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('output_nvc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: parallelize with C++ parallel algorithms and `views::cartesian_product`\n",
    "\n",
    "Same exercise as above, but instead of using linear-indexing, use `views::cartesian_product` for multi-dimensional iteration.\n",
    "\n",
    "A template for the solution is provided in [exercise1.cpp].\n",
    "\n",
    "[exercise1.cpp]: ./exercise1.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat exercise1.cpp -ltbb\n",
    "!./heat 1024 1024 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!clang++ -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat exercise1.cpp -ltbb\n",
    "!./heat 1024 1024 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions Exercise 1\n",
    "\n",
    "The solutions for each example are available in the `solutions/exercise1.cpp` sub-directory.\n",
    "\n",
    "The following compiles and runs the solutions for Exercise 1 using different compilers and C++ standard versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!g++ -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat solutions/exercise1.cpp -ltbb\n",
    "!./heat 1024 1024 2000\n",
    "!mv output output_gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!clang++ -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include -o heat solutions/exercise1.cpp -ltbb\n",
    "!./heat 1024 1024 20000\n",
    "!mv output output_clang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: MPI implementation\n",
    "\n",
    "The starting point for this exercise is [starting_point_mpi.cpp].\n",
    "\n",
    "[starting_point_mpi.cpp]: ./starting_point_mpi.cpp\n",
    "\n",
    "The goal of this exercise is to accelerate a pre-existing MPI implementation using parallel algorithms.\n",
    "\n",
    "The main differences with the previous examples is that the computation now involves a data exchange with neighbors, and that the computations have been split into:\n",
    "\n",
    "* `internal`: processes internal rows that do not depend on data from neighbors\n",
    "* `prev_boundary`: exchanges data with neighbor at `rank - 1` and processes the rows that depend on the elements received\n",
    "* `next_boundary`: exchanges data with neighbor at `rank + 1` and processes the rows that depend on the elements received\n",
    "\n",
    "\n",
    "```c++\n",
    "double internal(double* u_new, double* u_old, parameters p) {\n",
    "    grid g { .x_start = 2, .x_end = p.nx, .y_start = 1, .y_end = p.ny - 1 };\n",
    "    energy += stencil(u_new.get(), u_old.get(), g, p);\n",
    "}\n",
    "\n",
    "double prev_boundary(double* u_new, double* u_old, parameters p) {\n",
    "    // Send window cells, receive halo cells\n",
    "    if (p.rank > 0) {\n",
    "      // Send bottom boundary to bottom rank\n",
    "      MPI_Send(u_old + p.ny, p.ny, MPI_DOUBLE, p.rank - 1, 0, MPI_COMM_WORLD);\n",
    "      // Receive top boundary from bottom rank\n",
    "      MPI_Recv(u_old + 0, p.ny,  MPI_DOUBLE, p.rank - 1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "    }\n",
    "    grid g { .x_start = p.nx, .x_end = p.nx + 1, .y_start = 1, .y_end = p.ny - 1 };\n",
    "    return stencil(u_new, u_old, g, p);\n",
    "}\n",
    "\n",
    "double next_boundary(double* u_new, double* u_old, parameters p) {\n",
    "    if (p.rank < p.nranks - 1) {\n",
    "        // Receive bottom boundary from top rank\n",
    "        MPI_Recv(u_old + (p.nx + 1) * p.ny, p.ny, MPI_DOUBLE, p.rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "        // Send top boundary to top rank, and\n",
    "        MPI_Send(u_old + p.nx * p.ny, p.ny, MPI_DOUBLE, p.rank + 1, 1, MPI_COMM_WORLD);\n",
    "    }\n",
    "    grid g { .x_start = 1, .x_end = 2, .y_start = 1, .y_end = p.ny - 1 };\n",
    "    return stencil(u_new, u_old, g, p);\n",
    "}\n",
    "```\n",
    "\n",
    "You can parallelize this using the C++ parallel algorithms in the exact same way we have done above.\n",
    "Pick the approach that makes the most sense to you.\n",
    "\n",
    "A template for the solution is provided in [exercise2.cpp]. Notice that we only need to change the stencil function, there is nothing else required to accelerate an MPI application with C++.\n",
    "\n",
    "If you notice the performance of the GPU accelerated version degrade over the non-MPI version, see the slides for how to workaround this.\n",
    "\n",
    "[exercise2.cpp]: ./exercise2.cpp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat exercise2.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -stdpar=gpu  -gpu=cc80 -std=c++20 -fast -DNDEBUG -o heat exercise2.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_nvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Exercise 2\n",
    "\n",
    "The solutions for each example are available in the `solutions/exercise2.cpp` sub-directory and the `_nomanaged` variant that uses `thrust::device_vector` instead.\n",
    "\n",
    "The following compiles and runs the solutions for Exercise 2 using different compilers and C++ standard versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat solutions/exercise2.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -stdpar=gpu  -gpu=cc80 -std=c++20 -fast -DNDEBUG -o heat solutions/exercise2.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_nvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -stdpar=gpu  -gpu=cc80,nomanaged -std=c++20 -fast -DNDEBUG -o heat solutions/exercise2_nomanaged.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_nvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: overlapping communication and computation\n",
    "\n",
    "The goal of this exercise is to extend the solution of Exercise 2 to use multiple `std::thread`s to overlap computation with communication.\n",
    "\n",
    "The main `TODO`s are:\n",
    "\n",
    "* using `atomic` operations to modify the energy concurrently\n",
    "* using a `std::barrier` to synchronize the different threads\n",
    "* using one `std::thread` per computation\n",
    "* performing some of the operations in a critical section between the three threads:\n",
    "  * `MPI_Reduce` of the energy\n",
    "  * reset the energy to `0.` for the next iteration\n",
    "  \n",
    "See:\n",
    "\n",
    "\n",
    "```c++\n",
    "  // TODO: use a dynamically-allocated atomic variable for the energy\n",
    "  double* energy = new double{0.};\n",
    "    \n",
    "  // TODO: use a barrier for synchronization\n",
    "  // ...bar = ...\n",
    "\n",
    "  // TODO: use threads for the different computations\n",
    "  auto thread_prev = std::thread([/*TODO: complete capture */]() {\n",
    "      for (long it = 0; it < p.nit(); ++it) {\n",
    "          // TODO: perform the prev exchange and computation\n",
    "          // TODO: update the atomic energy\n",
    "          // TODO: synchronize with the barrier\n",
    "      }\n",
    "  });\n",
    "    \n",
    "  auto thread_next = /* TODO: similar for prev */;\n",
    "      \n",
    "  auto thread_internal = /*\n",
    "    TODO: same as for next and prev\n",
    "    TODO: need to perform the reduction in one of the threads (for example this one)\n",
    "    TODO: need to reset the atomic in one of the threads (for example this one)\n",
    "  */;\n",
    "\n",
    "  // TODO: join all threads\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat exercise3.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -stdpar=gpu  -gpu=cc80,nomanaged -std=c++20 -fast -DNDEBUG -o heat exercise3.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_nvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Exercise 3\n",
    "\n",
    "The solutions for each example are available in the `solutions/exercise3.cpp` sub-directory.\n",
    "\n",
    "The following compiles and runs the solutions for Exercise 3 using different compilers and C++ standard versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=g++ mpicxx -std=c++20 -Ofast -DNDEBUG -isystem/usr/local/range-v3/include  -o heat solutions/exercise3.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output || true\n",
    "!rm heat || true\n",
    "!OMPI_CXX=nvc++ mpicxx -stdpar=gpu  -gpu=cc80 -std=c++20 -fast -DNDEBUG -o heat solutions/exercise3.cpp -ltbb\n",
    "!OMPI_MCA_coll_hcoll_enable=0 mpirun --oversubscribe --allow-run-as-root -np 2 ./heat 1024 1024 2000\n",
    "!mv output output_nvc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
